{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfb11f5",
   "metadata": {},
   "source": [
    "# 0.3. Normalize and Crop Training Data\n",
    "\n",
    "Due to the large size of the training data, we will normalize and crop the images to a smaller size. This will help in reducing the computational load and make it easier to work with the data.\n",
    "\n",
    "Normalization is done to ensure that the pixel values are scaled to a range that is suitable for training machine learning models. Cropping is done to focus on the areas of interest in the images, which can help improve model performance by reducing noise and irrelevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b2780",
   "metadata": {},
   "source": [
    "## 0.3.1. Load Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929bc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from csbdeep.utils import normalize\n",
    "from tifffile import imread, imwrite, imshow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e1ad1",
   "metadata": {},
   "source": [
    "## 0.3.2. Load Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_coordinates_3d(\n",
    "    label_image: np.ndarray,\n",
    "    label_value: int = 1,\n",
    "    crop_size_xy: int = 128,\n",
    "    padding_z: int = 2,\n",
    "    min_crop_z: int = 12,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate the coordinates for a crop of specified size around the given label value in a 3D label image.\n",
    "    The z-dimension crop is dynamically determined based on the label's z-coordinates, with a minimum size enforced.\n",
    "\n",
    "    Parameters:\n",
    "        label_image (numpy.ndarray): The 3D label image.\n",
    "        label_value (int): The label value to center the crop around.\n",
    "        crop_size_xy (int): The size of the crop in the x and y dimensions.\n",
    "        padding_z (int): Extra slices to include above and below the label in the z-dimension.\n",
    "        min_crop_z (int): The minimum size of the crop in the z-dimension.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (start_z, start_x, start_y, end_z, end_x, end_y) coordinates for the crop.\n",
    "    \"\"\"\n",
    "    # Find the coordinates of the label with the specified value\n",
    "    coords = np.argwhere(label_image == label_value)\n",
    "\n",
    "    if coords.size == 0:\n",
    "        raise ValueError(f\"Label value {label_value} not found in the image.\")\n",
    "\n",
    "    # Calculate the z-range of the label\n",
    "    z_min = coords[:, 0].min()\n",
    "    z_max = coords[:, 0].max()\n",
    "\n",
    "    # Add padding to the z-range\n",
    "    start_z = max(z_min - padding_z, 0)\n",
    "    end_z = min(\n",
    "        z_max + padding_z + 1, label_image.shape[0]\n",
    "    )  # +1 to include the upper bound\n",
    "\n",
    "    # Ensure the z crop is at least `min_crop_z`\n",
    "    current_crop_z = end_z - start_z\n",
    "    if current_crop_z < min_crop_z:\n",
    "        extra = min_crop_z - current_crop_z\n",
    "        start_z = max(start_z - extra // 2, 0)\n",
    "        end_z = min(start_z + min_crop_z, label_image.shape[0])\n",
    "        start_z = max(0, end_z - min_crop_z)  # Adjust if crop exceeds bounds\n",
    "\n",
    "    # Calculate the center of the label in the x and y dimensions\n",
    "    center_x = coords[:, 1].mean().astype(int)\n",
    "    center_y = coords[:, 2].mean().astype(int)\n",
    "\n",
    "    # Calculate crop boundaries for the x and y dimensions\n",
    "    half_crop_xy = crop_size_xy // 2\n",
    "\n",
    "    # X-dimension\n",
    "    start_x = max(center_x - half_crop_xy, 0)\n",
    "    end_x = min(start_x + crop_size_xy, label_image.shape[1])\n",
    "    start_x = max(0, end_x - crop_size_xy)  # Adjust if crop exceeds bounds\n",
    "\n",
    "    # Y-dimension\n",
    "    start_y = max(center_y - half_crop_xy, 0)\n",
    "    end_y = min(start_y + crop_size_xy, label_image.shape[2])\n",
    "    start_y = max(0, end_y - crop_size_xy)  # Adjust if crop exceeds bounds\n",
    "\n",
    "    return start_z, start_x, start_y, end_z, end_x, end_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105eb73d",
   "metadata": {},
   "source": [
    "## 0.3.3. Code to Normalize and Crop Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35819d5f",
   "metadata": {},
   "source": [
    "### User defined parameters:\n",
    "\n",
    "Directories for images and masks, both the original and watershed labels.\n",
    "\n",
    "The path to the images and masks should be specified in the variables `img_directory`, `og_mask_directory`, and `watershed_label_directory`. These directories should contain the respective image and mask files that you want to process.\n",
    "\n",
    "The original masks in `og_mask_directory` are used as reference for the cropping coordinates to reduce the generation of similiar cropped images, for example a cluster of touching watersheded labels . The watersheded labels in `watershed_label_directory` are used to generate the cropped labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the images and masks should be specified in the variables `img_directory`, and `watershed_label_directory`.\n",
    "img_directory = \"directory/to/images\"\n",
    "\n",
    "watershed_label_directory = \"directory/to/watershed/labels\"\n",
    "\n",
    "# OPTIONAL: If you have an original mask directory, specify it here.\n",
    "# This is used to reduce the amount of similiar crops generated by cropping each label in a cluster individually.\n",
    "og_mask_directory = \"directory/to/original/masks\"  # Set to None if not used\n",
    "\n",
    "# Provide the directories to store the cropped images and labels\n",
    "# They will be created if they do not exist\n",
    "cropped_img_directory = \"directory/to/cropped/images\"\n",
    "cropped_lbl_directory = \"directory/to/cropped/labels\"\n",
    "\n",
    "# Size of the crop in the x and y dimensions\n",
    "# Should be at least twice the patch size used for training the model\n",
    "# This is to ensure that an edge crop still has a sufficient size for training\n",
    "# For example, if the patch size is 128, a crop size of 256 is recommended.\n",
    "crop_size_xy = 256\n",
    "\n",
    "# Minimum size of the crop in the z-dimension\n",
    "min_crop_z = 30\n",
    "\n",
    "# Extra slices to include above and below the label in the z-dimension, only used if the z crop is bigger than the minimum size\n",
    "padding_z = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370b019",
   "metadata": {},
   "source": [
    "### Code to run:\n",
    "\n",
    "Important: Make sure to run the code in the cells below in order, as they depend on each other.\n",
    "\n",
    "- Corresponding images, masks, and labels must have the same file names.\n",
    "- The normalization is done on the whole image stack, not on individual slices. to reproduce the normalization, you can use the `normalize` function from `csbdeep.utils`, with axis set to (0,1,2) for 3D images or (0,1) for 2D images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a1b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: 08082024_rLabel_014.2_TAMRA_sense_P00002_C4scaled_oriScale.tif\n",
      "DONE: 08082024_rLabel_014.2_TAMRA_sense_P00006_C4scaled_oriScale.tif\n",
      "DONE: 08082024_rLabel_014.2_TAMRA_sense_P00009_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00013_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00017_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00020_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00023_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00027_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00031_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00033_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00045_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00049_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00051_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00052_C4scaled_oriScale.tif\n",
      "DONE: 21082024_rLabel_024_TAMRA_sense_P00055_C4scaled_oriScale.tif\n",
      "DONE: C3-03022022_Label49_t1_100x_0p21_02_POS_current_scaled_oriScale.tif\n",
      "DONE: C3-03022022_Label49_t1_100x_0p21_03_POS_current_scaled_oriScale.tif\n",
      "DONE: C3-03022022_Label49_t3_100x_0p21_03_POS_current_scaled_oriScale.tif\n",
      "DONE: C3-03022022_Label49_t3_100x_0p21_04_POS_current_scaled_oriScale.tif\n",
      "DONE: C3-26012022_Label48_t2_100x_0p21_03_POS_current_scaled_oriScale.tif\n",
      "DONE: C4-02122021_Label43_label1_343_0p25_100x_0p21_01_scaled_oriScale.tif\n",
      "DONE: C4-02122021_Label43_label1_343_0p25_100x_0p21_02_scaled_oriScale.tif\n",
      "DONE: C4-02122021_Label43_label1_343_0p25_100x_0p21_04_scaled_oriScale.tif\n",
      "DONE: C4-03022022_Label49_t2_100x_0p21_03_POS_current_scaled_oriScale.tif\n",
      "DONE: C4-03022022_Label49_t2_100x_0p21_04_POS_current_scaled_oriScale.tif\n",
      "DONE: C4-12012022_Label46_367L_BCd16low_100x_0p21_02_scaled_oriScale.tif\n",
      "DONE: C4-12012022_Label46_367L_Cd16_100x_0p21_01_scaled_oriScale.tif\n",
      "DONE: C4-12012022_Label46_367L_Cd16_100x_0p21_03_scaled_oriScale.tif\n",
      "DONE: C4-16122021_Label44_CD16_367L_w1_closetolabel_100x_0p21_01_scaled_oriScale.tif\n",
      "DONE: C4-16122021_Label44_CD16_367L_w1_closetolabel_100x_0p21_02_scaled_oriScale.tif\n",
      "DONE: C4-16122021_Label44_CD16_367L_w1_closetolabel_100x_0p21_03_scaled_oriScale.tif\n",
      "DONE: C4-16122021_Label44_CD16_367L_w1_closetolabel_100x_0p21_04_scaled_oriScale.tif\n",
      "DONE: C4-16122021_Label45_367L_w3_1076_100x_0p21_01_scaled_oriScale.tif\n",
      "DONE: C4-16122021_Label45_367L_w3_1076_100x_0p21_03_scaled_oriScale.tif\n",
      "DONE: C4-16122021_Label45_367L_w3_1077_100x_0p21_01_scaled_oriScale.tif\n",
      "DONE: C4-16122021_Label45_367L_w3_1079_100x_0p21_03_scaled_oriScale.tif\n",
      "DONE: C4-26012022_Label48_t1_100x_0p21_03_POS_current_scaled_oriScale.tif\n",
      "DONE: C4-26012022_Label48_t1strep_100x_0p21_02_POS_current_scaled_oriScale.tif\n",
      "DONE: C4-26012022_Label48_t1strep_100x_0p21_03_POS_current_scaled_oriScale.tif\n",
      "DONE: L72_w6_P00124_scaled_oriScale.tif\n",
      "DONE: L74_w9_P00101_scaled_oriScale.tif\n",
      "DONE: L74_w9_P00107_scaled_oriScale.tif\n",
      "DONE: rLabel_012.2_TAMRA_sense_P00019_C4scaled_oriScale.tif\n",
      "DONE: rLabel_012.2_TAMRA_sense_P00028_C4scaled_oriScale.tif\n",
      "DONE: rLabel_012.2_TAMRA_sense_P00033_C4scaled_oriScale.tif\n",
      "DONE: rLabel_012.2_TAMRA_sense_P00034_C4scaled_oriScale.tif\n",
      "DONE: rLabel_012.2_TAMRA_sense_P00041_C4scaled_oriScale.tif\n",
      "DONE: rLabel_012.2_TAMRA_sense_P00042_C4scaled_oriScale.tif\n"
     ]
    }
   ],
   "source": [
    "# Get the list of files in the specified image directory\n",
    "img_dir_list = sorted(os.listdir(img_directory))\n",
    "\n",
    "# Iterate through the file list and process each image\n",
    "for file in img_dir_list:\n",
    "    # only process files with .tif or .tiff extensions\n",
    "    if file.endswith((\".tif\", \".tiff\")):\n",
    "        img = imread(os.path.join(img_directory, file))\n",
    "        lbl = imread(os.path.join(watershed_label_directory, file))\n",
    "\n",
    "        # Handle different cases for og_mask_directory\n",
    "        try:\n",
    "            # Check if variable exists and is a valid directory path\n",
    "            if og_mask_directory and os.path.isdir(og_mask_directory):\n",
    "                mask = imread(os.path.join(og_mask_directory, file))\n",
    "            else:\n",
    "                mask = np.copy(lbl)\n",
    "        except (NameError, TypeError):\n",
    "            # Variable doesn't exist or is None\n",
    "            mask = np.copy(lbl)\n",
    "\n",
    "        # Normalize the image from 1 to 99.8 percentile\n",
    "        img = normalize(img, 1, 99.8, axis=(0, 1, 2))\n",
    "\n",
    "        # Ensure the directories for cropped images and labels exist\n",
    "        os.makedirs(cropped_img_directory, exist_ok=True)\n",
    "        os.makedirs(cropped_lbl_directory, exist_ok=True)\n",
    "\n",
    "        # Check if the mask has any labels\n",
    "        if mask.max() > 0:\n",
    "            # Iterate through each label in the mask, skipping the background (label 0)\n",
    "            for idx in range(1, mask.max() + 1):\n",
    "                # Get the 3D crop coordinates for the current label\n",
    "                crop_coords = get_crop_coordinates_3d(\n",
    "                    mask,\n",
    "                    label_value=idx,\n",
    "                    crop_size_xy=crop_size_xy,\n",
    "                    min_crop_z=min_crop_z,\n",
    "                    padding_z=padding_z,\n",
    "                )\n",
    "\n",
    "                # Crop the image and label using the calculated (z, x, y) coordinates\n",
    "                img_crop = img[\n",
    "                    crop_coords[0] : crop_coords[3],\n",
    "                    crop_coords[1] : crop_coords[4],\n",
    "                    crop_coords[2] : crop_coords[5],\n",
    "                ]\n",
    "                lbl_crop = lbl[\n",
    "                    crop_coords[0] : crop_coords[3],\n",
    "                    crop_coords[1] : crop_coords[4],\n",
    "                    crop_coords[2] : crop_coords[5],\n",
    "                ]\n",
    "\n",
    "                # Save the cropped image and label\n",
    "                imwrite(\n",
    "                    os.path.join(\n",
    "                        cropped_img_directory,\n",
    "                        file.split(\".tif\")[0] + \"_\" + str(idx) + \".tif\",\n",
    "                    ),\n",
    "                    img_crop,\n",
    "                )\n",
    "                imwrite(\n",
    "                    os.path.join(\n",
    "                        cropped_lbl_directory,\n",
    "                        file.split(\".tif\")[0] + \"_\" + str(idx) + \".tif\",\n",
    "                    ),\n",
    "                    lbl_crop,\n",
    "                )\n",
    "\n",
    "            print(f\"DONE: {file}\")\n",
    "\n",
    "print(\"All images processed and saved in the specified directories.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_region_props",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
